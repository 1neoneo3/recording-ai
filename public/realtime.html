<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>éŸ³å£°è‡ªå‹•æ–‡å­—èµ·ã“ã—</title>
    <link rel="icon" type="image/svg+xml" href="/favicon.svg">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Hiragino Sans', 'Noto Sans JP', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #fafafa;
            min-height: 100vh;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
        }
        
        h1 {
            font-size: 2.5rem;
            font-weight: 700;
            color: #1a1a1a;
            margin-bottom: 48px;
            text-align: center;
            letter-spacing: -0.02em;
        }
        
        .controls {
            display: flex;
            gap: 16px;
            justify-content: center;
            margin-bottom: 48px;
        }
        
        button {
            padding: 12px 32px;
            font-size: 15px;
            font-weight: 500;
            border: none;
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.2s ease;
            letter-spacing: 0.02em;
        }
        
        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        #startBtn {
            background-color: #1a1a1a;
            color: white;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        #startBtn:hover:not(:disabled) {
            background-color: #000;
            transform: translateY(-1px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }
        
        #stopBtn {
            background-color: #e5e5e5;
            color: #333;
        }
        
        #stopBtn:hover:not(:disabled) {
            background-color: #d5d5d5;
            transform: translateY(-1px);
        }
        
        #copyBtn {
            background-color: transparent;
            color: #666;
            border: 1px solid #e5e5e5;
            padding: 8px 20px;
            font-size: 14px;
        }
        
        #copyBtn:hover {
            background-color: #f5f5f5;
            border-color: #d5d5d5;
        }
        
        #copyBtn:active {
            background-color: #e5e5e5;
        }
        
        .status {
            background-color: white;
            border-radius: 12px;
            padding: 24px;
            margin-bottom: 32px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.04);
            border: 1px solid #f0f0f0;
        }
        
        .status.recording {
            border-color: #333;
        }
        
        .status.speaking {
            border-color: #666;
        }
        
        .model-selector {
            background-color: white;
            border-radius: 12px;
            padding: 20px;
            margin-bottom: 32px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.04);
            border: 1px solid #f0f0f0;
            text-align: center;
        }
        
        .model-selector label {
            font-size: 14px;
            color: #666;
            font-weight: 500;
            margin-right: 12px;
        }
        
        .model-selector select {
            padding: 10px 16px;
            font-size: 14px;
            border-radius: 8px;
            border: 1px solid #e5e5e5;
            background-color: white;
            cursor: pointer;
            min-width: 250px;
            transition: border-color 0.2s ease;
        }
        
        .model-selector select:hover {
            border-color: #ccc;
        }
        
        .model-selector select:focus {
            outline: none;
            border-color: #333;
        }
        
        .transcription-container {
            background-color: white;
            border-radius: 12px;
            padding: 32px;
            min-height: 400px;
            max-height: 600px;
            overflow-y: auto;
            box-shadow: 0 1px 3px rgba(0,0,0,0.04);
            border: 1px solid #f0f0f0;
        }
        
        .transcription-segment {
            margin-bottom: 24px;
            padding-bottom: 24px;
            border-bottom: 1px solid #f0f0f0;
            position: relative;
            padding-right: 40px;
        }
        
        .transcription-segment:last-child {
            border-bottom: none;
            margin-bottom: 0;
            padding-bottom: 0;
        }
        
        .transcription-segment.deleted {
            display: none;
        }
        
        .delete-btn {
            position: absolute;
            top: 0;
            right: 0;
            background: none;
            border: none;
            padding: 8px;
            cursor: pointer;
            color: #999;
            font-size: 16px;
            transition: color 0.2s ease;
            opacity: 0;
        }
        
        .transcription-segment:hover .delete-btn {
            opacity: 1;
        }
        
        .delete-btn:hover {
            color: #ff4444;
        }
        
        .timestamp {
            font-size: 12px;
            color: #999;
            margin-bottom: 8px;
            font-weight: 400;
        }
        
        .text {
            font-size: 16px;
            line-height: 1.8;
            color: #333;
            font-weight: 400;
        }
        
        .error {
            color: #666;
            font-size: 14px;
            padding: 16px;
            background-color: #f5f5f5;
            border-radius: 8px;
            margin-bottom: 24px;
            text-align: center;
            display: none;
        }
        
        .error:not(:empty) {
            display: block;
        }
        
        .results-header {
            display: flex;
            align-items: center;
            justify-content: space-between;
            margin-bottom: 24px;
        }
        
        .results-header h2 {
            font-size: 1.5rem;
            font-weight: 600;
            color: #1a1a1a;
            margin: 0;
        }
        
        @keyframes pulse {
            0% { opacity: 1; }
            50% { opacity: 0.5; }
            100% { opacity: 1; }
        }
        
        .recording-indicator {
            display: inline-block;
            width: 8px;
            height: 8px;
            background-color: #333;
            border-radius: 50%;
            margin-right: 8px;
            animation: pulse 1.5s infinite;
        }
        
        .speaking-indicator {
            display: inline-block;
            width: 8px;
            height: 8px;
            background-color: #666;
            border-radius: 50%;
            margin-right: 8px;
            animation: pulse 0.8s infinite;
        }
        
        #recordingStatus, #speakingStatus, #queueStatus {
            font-size: 14px;
            color: #666;
            font-weight: 500;
        }
        
        #queueStatus {
            display: flex;
            align-items: center;
            gap: 8px;
        }
        
        .queue-indicator {
            display: inline-block;
            padding: 2px 8px;
            background-color: #f0f0f0;
            border-radius: 12px;
            font-size: 12px;
            font-weight: 600;
            color: #333;
        }
        
        .queue-indicator.processing {
            background-color: #333;
            color: white;
        }
        
        #audioLevel {
            margin-top: 16px;
        }
        
        #audioLevel label {
            font-size: 14px;
            color: #666;
            margin-right: 8px;
        }
        
        #volumeMeter {
            -webkit-appearance: none;
            appearance: none;
            width: 200px;
            height: 6px;
            background: #f0f0f0;
            border-radius: 3px;
            overflow: hidden;
        }
        
        #volumeMeter::-webkit-progress-bar {
            background-color: #f0f0f0;
            border-radius: 3px;
        }
        
        #volumeMeter::-webkit-progress-value {
            background-color: #333;
            border-radius: 3px;
        }
        
        #volumeMeter::-moz-progress-bar {
            background-color: #333;
            border-radius: 3px;
        }
        
        #volumeValue {
            font-size: 14px;
            color: #666;
            margin-left: 8px;
            font-variant-numeric: tabular-nums;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>éŸ³å£°è‡ªå‹•æ–‡å­—èµ·ã“ã—</h1>
        
        <div class="controls">
            <button id="startBtn">éŒ²éŸ³é–‹å§‹</button>
            <button id="stopBtn" disabled>éŒ²éŸ³åœæ­¢</button>
        </div>
        
        <div class="model-selector">
            <label for="modelSelect">Whisperãƒ¢ãƒ‡ãƒ«</label>
            <select id="modelSelect">
                <option value="Xenova/whisper-tiny">Tiny (æœ€é€Ÿãƒ»ä½ç²¾åº¦)</option>
                <option value="Xenova/whisper-base">Base (é«˜é€Ÿãƒ»æ¨™æº–ç²¾åº¦)</option>
                <option value="Xenova/whisper-small">Small (æ¨™æº–é€Ÿåº¦ãƒ»è‰¯å¥½ç²¾åº¦)</option>
                <option value="Xenova/whisper-medium" selected>Medium (ã‚„ã‚„é…ã„ãƒ»é«˜ç²¾åº¦)</option>
                <option value="Xenova/whisper-large-v3">Large v3 (é…ã„ãƒ»æœ€é«˜ç²¾åº¦)</option>
                <option value="openai/whisper-turbo">ğŸš€ Turbo (Pythonç‰ˆãƒ»è¶…é«˜é€Ÿ)</option>
                <option value="openai/whisper-large-v3">ğŸ Large v3 (Pythonç‰ˆãƒ»æœ€é«˜ç²¾åº¦)</option>
                <option value="faster-whisper/tiny">âš¡ Faster Tiny (æœ€é€Ÿãƒ»ä½ç²¾åº¦)</option>
                <option value="faster-whisper/base">âš¡ Faster Base (è¶…é«˜é€Ÿãƒ»æ¨™æº–ç²¾åº¦)</option>
                <option value="faster-whisper/small">âš¡ Faster Small (é«˜é€Ÿãƒ»è‰¯å¥½ç²¾åº¦)</option>
                <option value="faster-whisper/medium">âš¡ Faster Medium (é€Ÿã„ãƒ»é«˜ç²¾åº¦)</option>
                <option value="faster-whisper/large-v2">âš¡ Faster Large v2 (æ¨™æº–é€Ÿåº¦ãƒ»æœ€é«˜ç²¾åº¦)</option>
                <option value="faster-whisper/large-v3">âš¡ Faster Large v3 (æ¨™æº–é€Ÿåº¦ãƒ»æœ€æ–°ãƒ¢ãƒ‡ãƒ«)</option>
            </select>
            
            <label for="concurrencySelect" style="margin-left: 20px;">ä¸¦åˆ—å‡¦ç†æ•°</label>
            <select id="concurrencySelect">
                <option value="1">1 (å®‰å®š)</option>
                <option value="2" selected>2 (æ¨™æº–)</option>
                <option value="3">3 (é«˜é€Ÿ)</option>
                <option value="4">4 (è¶…é«˜é€Ÿ)</option>
            </select>
        </div>
        
        <div id="status" class="status">
            <div id="recordingStatus">éŒ²éŸ³åœæ­¢ä¸­</div>
            <div id="speakingStatus"></div>
            <div id="queueStatus" style="margin-top: 8px;"></div>
            <div id="audioLevel" style="margin-top: 10px;">
                <label for="volumeMeter">éŸ³å£°ãƒ¬ãƒ™ãƒ«: </label>
                <progress id="volumeMeter" value="0" max="100" style="width: 200px;"></progress>
                <span id="volumeValue">0</span>
            </div>
        </div>
        
        <div id="error" class="error"></div>
        
        <div class="results-header">
            <h2>æ–‡å­—èµ·ã“ã—çµæœ</h2>
            <button id="copyBtn">å…¨æ–‡ã‚’ã‚³ãƒ”ãƒ¼</button>
        </div>
        <div id="transcription" class="transcription-container">
            <p style="color: #999; text-align: center; font-size: 14px;">éŒ²éŸ³ã‚’é–‹å§‹ã™ã‚‹ã¨ã€ã“ã“ã«æ–‡å­—èµ·ã“ã—çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚</p>
        </div>
    </div>

    <script>
        // Fallback VAD implementation if Hark doesn't load
        function createSimpleVAD(stream, options) {
            const audioContext = new (window.AudioContext || window.webkitAudioContext)();
            const analyser = audioContext.createAnalyser();
            const microphone = audioContext.createMediaStreamSource(stream);
            const scriptProcessor = audioContext.createScriptProcessor(2048, 1, 1);
            
            analyser.smoothingTimeConstant = 0.8;
            analyser.fftSize = 1024;
            
            microphone.connect(analyser);
            analyser.connect(scriptProcessor);
            scriptProcessor.connect(audioContext.destination);
            
            const handlers = {
                speaking: null,
                stopped_speaking: null
            };
            
            let speaking = false;
            let speakingHistory = [];
            const historyLength = 10;
            const threshold = options.threshold || -50;
            let fallbackSpeakingStartAt = null;
            
            scriptProcessor.onaudioprocess = function() {
                const array = new Uint8Array(analyser.frequencyBinCount);
                analyser.getByteFrequencyData(array);
                const values = array.reduce((a, b) => a + b, 0);
                const average = values / array.length;
                
                // Convert to dB
                const db = 20 * Math.log10(average / 128);
                
                speakingHistory.push(db > threshold);
                if (speakingHistory.length > historyLength) {
                    speakingHistory.shift();
                }
                
                const currentlySpeaking = speakingHistory.filter(v => v).length >= 3;
                
                if (currentlySpeaking && !speaking) {
                    speaking = true;
                    if (!fallbackSpeakingStartAt) {
                        fallbackSpeakingStartAt = Date.now();
                    }
                    if (handlers.speaking) handlers.speaking();
                } else if (!currentlySpeaking && speaking) {
                    speaking = false;
                    if (fallbackSpeakingStartAt) {
                        const duration = Date.now() - fallbackSpeakingStartAt;
                        if (duration >= SPEAKING_THRESHOLD) {
                            fallbackSpeakingStartAt = Date.now();
                            if (handlers.stopped_speaking) handlers.stopped_speaking();
                        }
                    }
                }
            };
            
            return {
                on: function(event, handler) {
                    handlers[event] = handler;
                },
                stop: function() {
                    microphone.disconnect();
                    analyser.disconnect();
                    scriptProcessor.disconnect();
                }
            };
        }
    </script>
    <script src="https://cdn.jsdelivr.net/npm/hark@1.2.3/hark.bundle.js" defer></script>
    <script type="module">
        let mediaRecorder = null;
        let audioStream = null;
        let speechEvents = null;
        let audioChunks = [];
        let isRecording = false;
        let isSpeaking = false;
        let segmentStartTime = null;
        let silenceTimeout = null;
        let chunkCounter = 0; // Counter for chunks
        let isProcessing = false; // Flag to prevent concurrent processing
        let periodicInterval = null; // Interval for periodic processing
        let speakingStartAt = null; // Track when speaking started
        const SPEAKING_THRESHOLD = 1000; // 1ç§’ã®ç™ºè©±ã§å‡¦ç†é–‹å§‹ï¼ˆãƒãƒ©ãƒ³ã‚¹èª¿æ•´ï¼‰
        let currentMimeType = null; // Store current mime type
        const processingQueue = []; // Queue for audio segments to process
        let isQueueProcessing = false; // Flag for queue processing
        let concurrentRequests = 0; // Track concurrent API requests
        let MAX_CONCURRENT_REQUESTS = 2; // Limit concurrent requests (now variable) - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’2ã«å¤‰æ›´
        // const MAX_QUEUE_SIZE = 4; // Maximum queue size (disabled - no limit)
        
        // Auto-stop on silence
        let lastSoundTime = Date.now(); // Track last time sound was detected
        let silenceCheckInterval = null; // Interval for checking silence duration
        const SILENCE_AUTO_STOP_DURATION = 5 * 60 * 1000; // 5 minutes in milliseconds
        
        // State persistence constants
        const STATE_KEY = 'recording_ai_state';
        const TRANSCRIPTION_KEY = 'recording_ai_transcriptions';
        const QUEUE_KEY = 'recording_ai_queue';
        const STATE_VERSION = '1.1';
        
        // Transcription segments with ordering
        const transcriptionSegments = new Map(); // Map<segmentId, {timestamp, text, order}>
        let segmentCounter = 0; // Global counter for segment ordering
        
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const statusDiv = document.getElementById('status');
        const recordingStatusDiv = document.getElementById('recordingStatus');
        const speakingStatusDiv = document.getElementById('speakingStatus');
        const queueStatusDiv = document.getElementById('queueStatus');
        const transcriptionDiv = document.getElementById('transcription');
        const errorDiv = document.getElementById('error');
        
        // Configuration
        const MIN_SEGMENT_DURATION = 500; // 0.5 second minimum for very fast response
        const MAX_SEGMENT_DURATION = 30000; // 30 seconds
        const SILENCE_THRESHOLD = 500; // 0.5 seconds for very quick detection
        // Note: SPEAKING_THRESHOLD is defined above as 10000ms (10 seconds)
        
        // State management functions
        function saveState() {
            const state = {
                version: STATE_VERSION,
                isRecording: isRecording,
                selectedModel: document.getElementById('modelSelect').value,
                concurrency: document.getElementById('concurrencySelect').value,
                timestamp: new Date().toISOString()
            };
            
            try {
                localStorage.setItem(STATE_KEY, JSON.stringify(state));
                console.log('State saved:', state);
            } catch (e) {
                console.error('Failed to save state:', e);
            }
        }
        
        function loadState() {
            try {
                const savedState = localStorage.getItem(STATE_KEY);
                if (!savedState) return null;
                
                const state = JSON.parse(savedState);
                if (state.version !== STATE_VERSION) {
                    console.log('State version mismatch, ignoring saved state');
                    return null;
                }
                
                console.log('State loaded:', state);
                return state;
            } catch (e) {
                console.error('Failed to load state:', e);
                return null;
            }
        }
        
        function saveTranscriptions() {
            // Only save non-deleted segments
            const transcriptionSegments = transcriptionDiv.querySelectorAll('.transcription-segment:not(.deleted)');
            const transcriptions = [];
            
            transcriptionSegments.forEach(segment => {
                const timestamp = segment.querySelector('.timestamp')?.textContent;
                const text = segment.querySelector('.text')?.textContent;
                
                if (timestamp && text && text !== 'å‡¦ç†ä¸­...') {
                    transcriptions.push({
                        timestamp: timestamp,
                        text: text
                    });
                }
            });
            
            try {
                localStorage.setItem(TRANSCRIPTION_KEY, JSON.stringify(transcriptions));
                console.log(`Saved ${transcriptions.length} transcriptions`);
            } catch (e) {
                console.error('Failed to save transcriptions:', e);
            }
        }
        
        function loadTranscriptions() {
            try {
                const savedTranscriptions = localStorage.getItem(TRANSCRIPTION_KEY);
                if (!savedTranscriptions) return;
                
                const transcriptions = JSON.parse(savedTranscriptions);
                console.log(`Loading ${transcriptions.length} saved transcriptions`);
                
                // Clear the default message
                if (transcriptionDiv.innerHTML.includes('éŒ²éŸ³ã‚’é–‹å§‹ã™ã‚‹ã¨')) {
                    transcriptionDiv.innerHTML = '';
                }
                
                // Restore transcriptions
                transcriptions.forEach(({ timestamp, text }, index) => {
                    const segmentId = ++segmentCounter;
                    const segment = document.createElement('div');
                    segment.className = 'transcription-segment';
                    segment.setAttribute('data-segment-id', segmentId);
                    
                    const timestampDiv = document.createElement('div');
                    timestampDiv.className = 'timestamp';
                    timestampDiv.textContent = timestamp;
                    
                    const textDiv = document.createElement('div');
                    textDiv.className = 'text';
                    textDiv.textContent = text;
                    
                    const deleteBtn = document.createElement('button');
                    deleteBtn.className = 'delete-btn';
                    deleteBtn.innerHTML = 'ğŸ—‘ï¸';
                    deleteBtn.title = 'å‰Šé™¤';
                    deleteBtn.onclick = function() {
                        deleteTranscriptionSegment(segmentId);
                    };
                    
                    segment.appendChild(timestampDiv);
                    segment.appendChild(textDiv);
                    segment.appendChild(deleteBtn);
                    transcriptionDiv.appendChild(segment);
                    
                    // Add to transcriptionSegments map
                    transcriptionSegments.set(segmentId, {
                        timestamp: timestamp,
                        text: text,
                        order: segmentId
                    });
                });
                
                // Scroll to bottom
                transcriptionDiv.scrollTop = transcriptionDiv.scrollHeight;
            } catch (e) {
                console.error('Failed to load transcriptions:', e);
            }
        }
        
        function clearSavedState() {
            try {
                localStorage.removeItem(STATE_KEY);
                localStorage.removeItem(TRANSCRIPTION_KEY);
                localStorage.removeItem(QUEUE_KEY);
                console.log('Saved state cleared');
            } catch (e) {
                console.error('Failed to clear saved state:', e);
            }
        }
        
        function saveQueue() {
            try {
                // Convert queue items to serializable format
                const queueData = processingQueue.map(item => ({
                    id: item.id,
                    timestamp: item.timestamp.toISOString(),
                    chunkCount: item.chunkCount,
                    blobSize: item.blob.size,
                    blobType: item.blob.type,
                    order: item.order
                }));
                
                localStorage.setItem(QUEUE_KEY, JSON.stringify({
                    queue: queueData,
                    isProcessing: isQueueProcessing
                }));
                
                console.log(`Saved queue state: ${queueData.length} items`);
            } catch (e) {
                console.error('Failed to save queue:', e);
            }
        }
        
        function loadQueue() {
            try {
                const savedQueue = localStorage.getItem(QUEUE_KEY);
                if (!savedQueue) return;
                
                const queueData = JSON.parse(savedQueue);
                console.log(`Found saved queue with ${queueData.queue.length} items`);
                
                // Note: We can't restore the actual audio blobs from localStorage
                // But we can show that there were pending items
                if (queueData.queue.length > 0) {
                    console.warn(`${queueData.queue.length} audio segments were lost due to page reload`);
                    
                    // Update queue status to show lost items
                    const lostItemsMsg = document.createElement('div');
                    lostItemsMsg.style.cssText = `
                        background: #fff3cd;
                        color: #856404;
                        padding: 12px 16px;
                        margin: 16px 0;
                        border-radius: 8px;
                        font-size: 14px;
                        text-align: center;
                    `;
                    lostItemsMsg.textContent = `${queueData.queue.length}ä»¶ã®éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒãƒªãƒ­ãƒ¼ãƒ‰ã«ã‚ˆã‚Šå¤±ã‚ã‚Œã¾ã—ãŸ`;
                    
                    // Insert after status div
                    statusDiv.parentNode.insertBefore(lostItemsMsg, statusDiv.nextSibling);
                    
                    // Remove message after 5 seconds
                    setTimeout(() => lostItemsMsg.remove(), 5000);
                    
                    // Restore segment counter to maintain order
                    const maxOrder = Math.max(...queueData.queue.map(item => item.order || 0), segmentCounter);
                    segmentCounter = maxOrder;
                    console.log(`Restored segment counter to: ${segmentCounter}`);
                }
                
                // Clear the saved queue since we can't restore it
                localStorage.removeItem(QUEUE_KEY);
            } catch (e) {
                console.error('Failed to load queue:', e);
            }
        }
        
        async function startRecording() {
            try {
                errorDiv.textContent = '';
                
                // Get microphone access with simplified settings
                audioStream = await navigator.mediaDevices.getUserMedia({ 
                    audio: true 
                });
                
                // Set up audio level monitoring
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const analyser = audioContext.createAnalyser();
                const microphone = audioContext.createMediaStreamSource(audioStream);
                const scriptProcessor = audioContext.createScriptProcessor(2048, 1, 1);
                
                analyser.smoothingTimeConstant = 0.8;
                analyser.fftSize = 1024;
                
                microphone.connect(analyser);
                analyser.connect(scriptProcessor);
                scriptProcessor.connect(audioContext.destination);
                
                scriptProcessor.onaudioprocess = function() {
                    const array = new Uint8Array(analyser.frequencyBinCount);
                    analyser.getByteFrequencyData(array);
                    const values = array.reduce((a, b) => a + b, 0);
                    const average = values / array.length;
                    
                    // Update volume meter
                    document.getElementById('volumeMeter').value = average;
                    document.getElementById('volumeValue').textContent = Math.round(average);
                };
                
                // Initialize MediaRecorder
                const mimeType = getSupportedMimeType();
                currentMimeType = mimeType; // Store mime type
                mediaRecorder = new MediaRecorder(audioStream, {
                    mimeType,
                    audioBitsPerSecond: 256000 // Higher bitrate for better quality
                });
                
                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                        chunkCounter++;
                        console.log(`Audio chunk received #${chunkCounter}, size: ${event.data.size}, total chunks: ${audioChunks.length}`);
                        
                        // Update last sound time when receiving audio data
                        if (event.data.size > 1000) { // Only count chunks with meaningful audio
                            lastSoundTime = Date.now();
                        }
                        
                        // Process every 2 chunks (approximately 4 seconds with 2-second chunks) for faster response
                        if (chunkCounter >= 2) {
                            console.log('Processing 2 chunks automatically');
                            // Need to stop and restart recorder for complete WebM file
                            restartRecorderForChunks();
                        }
                    }
                };
                
                mediaRecorder.onerror = (error) => {
                    console.error('MediaRecorder error:', error);
                    errorDiv.textContent = 'Recording error: ' + error.message;
                };
                
                // Initialize Voice Activity Detection
                const options = {
                    threshold: -70, // More sensitive threshold
                    interval: 50,
                    history: 10,
                    smoothing: 0.1
                };
                
                // Use Hark if available, otherwise use fallback
                if (typeof window.hark !== 'undefined') {
                    speechEvents = window.hark(audioStream, options);
                    console.log('Using Hark for VAD');
                } else {
                    console.log('Hark not available, using fallback VAD');
                    speechEvents = createSimpleVAD(audioStream, options);
                }
                
                speechEvents.on('speaking', () => {
                    console.log('Speaking started');
                    isSpeaking = true;
                    
                    // Update last sound time
                    lastSoundTime = Date.now();
                    
                    // Record when speaking started if not already tracking
                    if (!speakingStartAt) {
                        speakingStartAt = Date.now();
                        console.log('Started tracking speaking duration');
                    }
                    
                    updateStatus();
                    
                    // Clear any pending silence timeout
                    if (silenceTimeout) {
                        clearTimeout(silenceTimeout);
                        silenceTimeout = null;
                    }
                });
                
                speechEvents.on('stopped_speaking', () => {
                    console.log('Speaking stopped');
                    isSpeaking = false;
                    updateStatus();
                    
                    if (!speakingStartAt) {
                        return;
                    }
                    
                    // Calculate speaking duration
                    const speakingDuration = Date.now() - speakingStartAt;
                    console.log(`Speaking duration: ${(speakingDuration / 1000).toFixed(1)}s`);
                    
                    // Check if speaking duration meets threshold
                    if (speakingDuration < SPEAKING_THRESHOLD) {
                        console.log(`Duration too short (${(speakingDuration / 1000).toFixed(1)}s < ${SPEAKING_THRESHOLD / 1000}s), continuing...`);
                        return;
                    }
                    
                    // Reset speaking start time
                    speakingStartAt = Date.now();
                    
                    // Process the segment by restarting recorder
                    console.log('Speaking duration threshold met, processing segment');
                    
                    // Only restart if we have meaningful audio chunks
                    if (audioChunks.length > 0) {
                        // Restart recorder to create complete WebM file
                        restartRecorderForChunks();
                    }
                });
                
                // Start recording with 2 second chunks for better stability
                mediaRecorder.start(2000); // Collect data every 2 seconds
                isRecording = true;
                segmentStartTime = Date.now();
                audioChunks = [];
                
                // Clear previous transcriptions or initial message
                if (transcriptionDiv.innerHTML.includes('éŒ²éŸ³ã‚’é–‹å§‹ã™ã‚‹ã¨')) {
                    transcriptionDiv.innerHTML = '';
                }
                
                // Reset chunk counter and speaking start time
                chunkCounter = 0;
                speakingStartAt = null;
                
                // Update UI
                startBtn.disabled = true;
                stopBtn.disabled = false;
                updateStatus();
                
                console.log('Recording started');
                
                // Save state after starting recording
                saveState();
                
                // Start silence check interval
                lastSoundTime = Date.now(); // Initialize last sound time
                silenceCheckInterval = setInterval(() => {
                    const silenceDuration = Date.now() - lastSoundTime;
                    if (silenceDuration >= SILENCE_AUTO_STOP_DURATION) {
                        console.log(`Auto-stopping recording after ${SILENCE_AUTO_STOP_DURATION / 1000 / 60} minutes of silence`);
                        stopRecording();
                        
                        // Show notification to user
                        const notification = document.createElement('div');
                        notification.style.cssText = `
                            position: fixed;
                            top: 20px;
                            right: 20px;
                            background: #ff9800;
                            color: white;
                            padding: 16px 24px;
                            border-radius: 8px;
                            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
                            z-index: 1000;
                            font-weight: 500;
                        `;
                        notification.textContent = '5åˆ†é–“ç„¡éŸ³ãŒç¶šã„ãŸãŸã‚ã€éŒ²éŸ³ã‚’è‡ªå‹•åœæ­¢ã—ã¾ã—ãŸ';
                        document.body.appendChild(notification);
                        
                        setTimeout(() => {
                            notification.remove();
                        }, 5000);
                    }
                }, 10000); // Check every 10 seconds
                
                // Disable periodic processing - rely on VAD only
                // periodicInterval = setInterval(() => {
                //     if (!isProcessing && audioChunks.length > 0) {
                //         console.log('Periodic processing triggered');
                //         // Stop and restart MediaRecorder to get complete WebM file
                //         restartRecordingForProcessing();
                //     }
                // }, 5000); // Every 5 seconds
                
            } catch (error) {
                console.error('Error starting recording:', error);
                errorDiv.textContent = 'Failed to start recording: ' + error.message;
            }
        }
        
        // This function is no longer needed with the new queue system
        /*
        async function restartRecordingForProcessing() {
            if (!mediaRecorder || mediaRecorder.state !== 'recording') return;
            
            console.log('Restarting recorder for processing...');
            
            // Stop current recording
            mediaRecorder.stop();
            
            // Wait for the stop event to fire and process data
            mediaRecorder.onstop = async () => {
                // Process the accumulated chunks
                if (audioChunks.length > 0) {
                    await processSegment();
                }
                
                // Restart recording with same settings
                if (isRecording && audioStream) {
                    mediaRecorder = new MediaRecorder(audioStream, {
                        mimeType: currentMimeType,
                        audioBitsPerSecond: 256000
                    });
                    
                    mediaRecorder.ondataavailable = (event) => {
                        if (event.data.size > 0) {
                            audioChunks.push(event.data);
                            chunkCounter++;
                            console.log(`Audio chunk received #${chunkCounter}, size: ${event.data.size}, total chunks: ${audioChunks.length}`);
                        }
                    };
                    
                    mediaRecorder.onerror = (error) => {
                        console.error('MediaRecorder error:', error);
                        errorDiv.textContent = 'Recording error: ' + error.message;
                    };
                    
                    // Start recording again with 2 second chunks
                    mediaRecorder.start(2000);
                    console.log('Recording restarted');
                }
            };
        }
        */
        
        // Function to restart recorder for chunk processing
        function restartRecorderForChunks() {
            if (!mediaRecorder || mediaRecorder.state !== 'recording') {
                console.log(`MediaRecorder not in recording state (state: ${mediaRecorder?.state}), skipping restart`);
                return;
            }
            
            console.log(`Restarting recorder for chunk processing... (state: ${mediaRecorder.state})`);
            
            // Set a flag to prevent recursive calls
            if (mediaRecorder.isRestarting) {
                console.log('Already restarting, skipping');
                return;
            }
            mediaRecorder.isRestarting = true;
            
            // Set up one-time onstop handler for chunk processing
            mediaRecorder.onstop = () => {
                console.log('Recorder stopped for chunk processing');
                
                // Add small delay to ensure all data is flushed
                setTimeout(() => {
                    // Process the chunks
                    if (audioChunks.length > 0) {
                        const chunksToProcess = [...audioChunks];
                        console.log(`Processing ${chunksToProcess.length} chunks`);
                        enqueueSegment(chunksToProcess, new Date());
                        
                        // Clear current chunks and reset counter
                        audioChunks = [];
                        chunkCounter = 0;
                        segmentStartTime = Date.now();
                    }
                
                // Restart recording if still active
                if (isRecording && audioStream) {
                    // No delay - restart immediately
                    mediaRecorder = new MediaRecorder(audioStream, {
                        mimeType: currentMimeType,
                        audioBitsPerSecond: 256000
                    });
                        
                        mediaRecorder.ondataavailable = (event) => {
                            if (event.data.size > 0) {
                                audioChunks.push(event.data);
                                chunkCounter++;
                                console.log(`Audio chunk received #${chunkCounter}, size: ${event.data.size}, total chunks: ${audioChunks.length}`);
                                
                                // Update last sound time when receiving audio data
                                if (event.data.size > 1000) { // Only count chunks with meaningful audio
                                    lastSoundTime = Date.now();
                                }
                                
                                // Process every 2 chunks (approximately 4 seconds with 2-second chunks) for faster response
                                if (chunkCounter >= 2) {
                                    console.log('Processing 2 chunks automatically');
                                    restartRecorderForChunks();
                                }
                            }
                        };
                        
                        mediaRecorder.onerror = (error) => {
                            console.error('MediaRecorder error:', error);
                            errorDiv.textContent = 'Recording error: ' + error.message;
                        };
                        
                    // Start recording again with 2 second chunks
                    mediaRecorder.start(2000);
                    mediaRecorder.isRestarting = false; // Reset the flag
                    console.log('Recording restarted after chunk processing');
                }
                }, 100); // 100ms delay to ensure data is flushed
            };
            
            // Stop the recorder
            if (mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
            }
        }
        
        async function stopRecording() {
            if (!isRecording) return;
            
            // Stop VAD
            if (speechEvents) {
                speechEvents.stop();
                speechEvents = null;
            }
            
            // Stop MediaRecorder
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
                
                // Process any remaining audio
                if (audioChunks.length > 0) {
                    await processSegment();
                }
            }
            
            // Stop audio stream
            if (audioStream) {
                audioStream.getTracks().forEach(track => track.stop());
                audioStream = null;
            }
            
            // Clear timeout
            if (silenceTimeout) {
                clearTimeout(silenceTimeout);
                silenceTimeout = null;
            }
            
            // Clear silence check interval
            if (silenceCheckInterval) {
                clearInterval(silenceCheckInterval);
                silenceCheckInterval = null;
            }
            
            // Clear periodic interval
            if (periodicInterval) {
                clearInterval(periodicInterval);
                periodicInterval = null;
            }
            
            isRecording = false;
            isSpeaking = false;
            mediaRecorder = null;
            audioChunks = [];
            
            // Update UI
            startBtn.disabled = false;
            stopBtn.disabled = true;
            updateStatus();
            
            console.log('Recording stopped');
            
            // Save state after stopping recording
            saveState();
        }
        
        // This function is no longer used with the new 10-second threshold approach
        /*
        async function checkAndProcessSegment() {
            const segmentDuration = Date.now() - segmentStartTime;
            
            // Check minimum duration
            if (segmentDuration >= MIN_SEGMENT_DURATION) {
                await processSegment();
            } else {
                console.log(`Segment too short (${(segmentDuration / 1000).toFixed(1)}s), waiting for more audio`);
            }
            
            // Force process if maximum duration reached
            if (segmentDuration >= MAX_SEGMENT_DURATION) {
                await processSegment();
            }
        }
        */
        
        // Add audio segment to processing queue
        function enqueueSegment(chunks, timestamp) {
            if (chunks.length === 0) return;
            
            // Create blob with proper mime type
            const mimeType = currentMimeType || 'audio/webm;codecs=opus';
            const audioBlob = new Blob(chunks, { type: mimeType });
            
            // Validate blob size
            if (audioBlob.size === 0) {
                console.warn('Empty audio blob, skipping');
                return;
            }
            
            // Skip very small blobs that might be corrupted
            if (audioBlob.size < 1000) {
                console.warn(`Audio blob too small (${audioBlob.size} bytes), might be corrupted, skipping`);
                return;
            }
            
            // Log blob details for debugging
            console.log(`Audio blob details: size=${audioBlob.size}, type=${audioBlob.type}, chunks=${chunks.length}`);
            
            const segmentId = ++segmentCounter; // Increment and get new segment ID
            
            processingQueue.push({
                id: segmentId,
                blob: audioBlob,
                timestamp: timestamp || new Date(),
                chunkCount: chunks.length,
                order: segmentId // Use segmentId as order
            });
            
            console.log(`Enqueued segment: ${chunks.length} chunks, blob size: ${audioBlob.size} bytes, queue size: ${processingQueue.length}`);
            
            // Calculate total memory usage in queue
            const totalMemory = processingQueue.reduce((sum, item) => sum + (item.blob ? item.blob.size : 0), 0);
            console.log(`Total queue memory: ${(totalMemory / 1024 / 1024).toFixed(2)} MB`);
            
            // Update queue status
            updateQueueStatus();
            
            // Save queue state
            saveQueue();
            
            // Start processing queue immediately
            // Allow multiple concurrent processQueue calls up to the limit
            if (concurrentRequests < MAX_CONCURRENT_REQUESTS) {
                setTimeout(() => processQueue(), 0);
            }
        }
        
        // Process queued segments with improved concurrency
        async function processQueue() {
            if (processingQueue.length === 0) return;
            
            updateQueueStatus();
            
            // Check if we can process more segments
            if (concurrentRequests >= MAX_CONCURRENT_REQUESTS) {
                console.log('Max concurrent requests reached, waiting...');
                return;
            }
            
            // Process one segment at a time to prevent race conditions
            const segment = processingQueue.shift();
            if (!segment) return;
            
            updateQueueStatus();
            saveQueue(); // Save after removing from queue
            
            // Process segment without waiting (allow concurrent processing)
            processSegmentData(segment).catch(error => {
                console.error('Unexpected error in processSegmentData:', error);
            }).finally(() => {
                // Check if more segments need processing
                if (processingQueue.length > 0) {
                    // Use setImmediate equivalent for faster processing
                    setTimeout(() => processQueue(), 0);
                }
            });
            
            // Try to process more segments if we haven't reached the limit
            if (processingQueue.length > 0 && concurrentRequests < MAX_CONCURRENT_REQUESTS - 1) {
                // Small delay to prevent race condition
                setTimeout(() => processQueue(), 10);
            }
            
            updateQueueStatus();
            saveQueue(); // Save final state
        }
        
        // Process individual segment data
        async function processSegmentData(segment, retryCount = 0) {
            // Increment concurrent request counter
            concurrentRequests++;
            console.log(`Processing audio segment (concurrent: ${concurrentRequests})...`);
            console.log('Audio blob size:', segment.blob.size, 'bytes');
            console.log('Timestamp:', segment.timestamp.toLocaleTimeString('ja-JP'));
            if (retryCount > 0) {
                console.log(`Retry attempt ${retryCount}`);
            }
            
            // Show processing indicator with segment ID
            const processingDiv = document.createElement('div');
            processingDiv.className = 'transcription-segment';
            processingDiv.style.opacity = '0.6';
            processingDiv.setAttribute('data-segment-id', segment.id);
            processingDiv.innerHTML = '<div class="timestamp">' + segment.timestamp.toLocaleTimeString('ja-JP') + '</div>' +
                                     '<div class="text" style="color: #666;">å‡¦ç†ä¸­...</div>';
            
            console.log('Created processingDiv for segment:', segment.id);
            
            // Insert in correct position based on segment order
            insertSegmentInOrder(processingDiv, segment.order);
            
            console.log('ProcessingDiv inserted, parent:', processingDiv.parentNode ? 'exists' : 'null');
            
            try {
                // Send to server for transcription
                const formData = new FormData();
                formData.append('audio', segment.blob, 'segment.webm');
                
                // Add selected model to form data
                const modelSelect = document.getElementById('modelSelect');
                formData.append('model', modelSelect.value);
                
                console.log(`Sending to /api/transcribe with model: ${modelSelect.value}`);
                console.log(`Audio blob details: size=${segment.blob.size}, type=${segment.blob.type}`);
                
                // Don't clear blob reference yet - we might need it for retry
                // segment.blob = null;
                
                const response = await fetch('/api/transcribe', {
                    method: 'POST',
                    body: formData
                });
                
                console.log('Response status:', response.status);
                console.log('Response headers:', response.headers.get('content-type'));
                
                if (!response.ok) {
                    const errorText = await response.text();
                    throw new Error(`Transcription failed: ${response.statusText} - ${errorText}`);
                }
                
                const result = await response.json();
                console.log('Full transcription result:', JSON.stringify(result, null, 2));
                
                // Display transcription
                console.log('Result type:', typeof result);
                console.log('Result keys:', Object.keys(result));
                
                // Handle both direct text and nested text property
                let transcriptionText = '';
                if (typeof result === 'string') {
                    transcriptionText = result;
                } else if (result.text) {
                    transcriptionText = result.text;
                } else if (result.transcription) {
                    transcriptionText = result.transcription;
                }
                
                console.log('Extracted text:', transcriptionText);
                
                if (transcriptionText) {
                    const cleanedText = transcriptionText.trim();
                    console.log('Cleaned text:', cleanedText);
                    if (cleanedText && cleanedText !== '[BLANK_AUDIO]' && !cleanedText.includes('[MUSIC]') && !cleanedText.includes('[static]') && !cleanedText.includes('[silence]')) {
                        // Update processing indicator with transcription
                        if (processingDiv && processingDiv.parentNode) {
                            // Store transcription in map
                            transcriptionSegments.set(segment.id, {
                                timestamp: segment.timestamp,
                                text: cleanedText,
                                order: segment.order
                            });
                            
                            // Update the processing div with actual transcription
                            processingDiv.style.opacity = '1';
                            const textDiv = processingDiv.querySelector('.text');
                            if (textDiv) {
                                textDiv.style.color = '#333';
                                textDiv.textContent = cleanedText;
                            }
                            
                            // Add delete button
                            if (!processingDiv.querySelector('.delete-btn')) {
                                const deleteBtn = document.createElement('button');
                                deleteBtn.className = 'delete-btn';
                                deleteBtn.innerHTML = 'ğŸ—‘ï¸';
                                deleteBtn.title = 'å‰Šé™¤';
                                deleteBtn.onclick = function() {
                                    deleteTranscriptionSegment(segment.id);
                                };
                                processingDiv.appendChild(deleteBtn);
                            }
                            
                            // Save transcriptions after update
                            saveTranscriptions();
                            
                            console.log('Transcription updated for segment:', segment.id);
                        } else {
                            console.error('ProcessingDiv not found for segment:', segment.id);
                        }
                    } else {
                        console.log('Skipping empty or noise transcription:', cleanedText);
                        // Remove processing indicator for empty/noise transcription
                        if (processingDiv && processingDiv.parentNode) {
                            transcriptionDiv.removeChild(processingDiv);
                        }
                    }
                } else {
                    console.log('No text found in transcription result');
                    // Remove processing indicator if no text found
                    if (processingDiv && processingDiv.parentNode) {
                        transcriptionDiv.removeChild(processingDiv);
                    }
                }
                
                // Clear the blob to free memory
                if (segment.blob) {
                    segment.blob = null; // Release blob reference
                }
                
                // Reset for next segment
                audioChunks = [];
                segmentStartTime = Date.now();
                
                // Note: chunkCounter is reset in ondataavailable handler
                
            } catch (error) {
                console.error('Error processing segment:', error);
                
                // Remove processing indicator on error
                if (processingDiv && processingDiv.parentNode) {
                    transcriptionDiv.removeChild(processingDiv);
                }
                
                // Clear the blob to free memory on error
                if (segment.blob) {
                    segment.blob = null; // Release blob reference
                }
                
                // Retry logic for conversion errors or connection errors
                if ((error.message.includes('Failed to convert audio to WAV') || 
                     error.message.includes('Failed to fetch') || 
                     error.message.includes('ERR_CONNECTION_REFUSED')) && retryCount < 3) {
                    console.log(`Error occurred, retrying in 500ms... (attempt ${retryCount + 1}/3)`);
                    // Decrement counter before retry
                    concurrentRequests--;
                    // Wait 500ms before retry for connection issues
                    await new Promise(resolve => setTimeout(resolve, 500));
                    return processSegmentData(segment, retryCount + 1);
                }
                
                // Show error but continue processing
                console.warn('Segment processing failed, continuing with next segment...');
                errorDiv.textContent = 'Some segments failed: ' + error.message;
                
                // Clear error after 3 seconds
                setTimeout(() => {
                    if (errorDiv.textContent.includes('Some segments failed')) {
                        errorDiv.textContent = '';
                    }
                }, 3000);
            } finally {
                // Always decrement concurrent request counter
                concurrentRequests--;
                console.log(`Request completed (concurrent: ${concurrentRequests})`);
            }
        }
        
        // Modified processSegment to use queue
        async function processSegment() {
            if (audioChunks.length === 0) return;
            
            console.log('Adding segment to queue...');
            console.log('Audio chunks:', audioChunks.length);
            console.log('Segment duration:', ((Date.now() - segmentStartTime) / 1000).toFixed(1) + 's');
            
            // Add to queue instead of processing directly
            enqueueSegment([...audioChunks], new Date());
            
            // Reset for next segment
            audioChunks = [];
            segmentStartTime = Date.now();
        }
        
        // Insert segment in correct order based on segment ID
        function insertSegmentInOrder(segmentDiv, order) {
            const existingSegments = transcriptionDiv.querySelectorAll('.transcription-segment');
            let inserted = false;
            
            for (const existing of existingSegments) {
                const existingId = parseInt(existing.getAttribute('data-segment-id') || '0');
                if (existingId > order) {
                    existing.parentNode.insertBefore(segmentDiv, existing);
                    inserted = true;
                    break;
                }
            }
            
            if (!inserted) {
                transcriptionDiv.appendChild(segmentDiv);
            }
            
            // Scroll to bottom
            transcriptionDiv.scrollTop = transcriptionDiv.scrollHeight;
        }
        
        function addTranscriptionSegment(text) {
            // This function is now mostly replaced by the order-preserving logic
            // but kept for backward compatibility
            const segmentId = ++segmentCounter;
            const segment = document.createElement('div');
            segment.className = 'transcription-segment';
            segment.setAttribute('data-segment-id', segmentId);
            
            const timestamp = document.createElement('div');
            timestamp.className = 'timestamp';
            timestamp.textContent = new Date().toLocaleTimeString('ja-JP');
            
            const textDiv = document.createElement('div');
            textDiv.className = 'text';
            textDiv.textContent = text;
            
            const deleteBtn = document.createElement('button');
            deleteBtn.className = 'delete-btn';
            deleteBtn.innerHTML = 'ğŸ—‘ï¸';
            deleteBtn.title = 'å‰Šé™¤';
            deleteBtn.onclick = function() {
                deleteTranscriptionSegment(segmentId);
            };
            
            segment.appendChild(timestamp);
            segment.appendChild(textDiv);
            segment.appendChild(deleteBtn);
            
            insertSegmentInOrder(segment, segmentId);
            
            // Save transcriptions after adding new segment
            saveTranscriptions();
        }
        
        // Delete a transcription segment
        function deleteTranscriptionSegment(segmentId) {
            const segment = document.querySelector(`[data-segment-id="${segmentId}"]`);
            if (segment) {
                // Add deleted class instead of removing from DOM
                segment.classList.add('deleted');
                
                // Remove from transcriptionSegments map
                transcriptionSegments.delete(segmentId);
                
                // Save updated transcriptions
                saveTranscriptions();
                
                console.log('Deleted segment:', segmentId);
            }
        }
        
        function updateStatus() {
            statusDiv.className = 'status';
            
            if (isRecording) {
                statusDiv.classList.add('recording');
                recordingStatusDiv.innerHTML = '<span class="recording-indicator"></span>éŒ²éŸ³ä¸­';
            } else {
                recordingStatusDiv.textContent = 'éŒ²éŸ³åœæ­¢ä¸­';
            }
            
            if (isSpeaking) {
                statusDiv.classList.add('speaking');
                speakingStatusDiv.innerHTML = '<span class="speaking-indicator"></span>ç™ºè©±æ¤œå‡ºä¸­';
            } else {
                speakingStatusDiv.textContent = '';
            }
            
            // Update queue status
            updateQueueStatus();
        }
        
        function updateQueueStatus() {
            const queueLength = processingQueue.length;
            
            if (queueLength === 0 && !isQueueProcessing) {
                queueStatusDiv.innerHTML = '';
            } else {
                let statusHTML = '';
                
                if (isQueueProcessing) {
                    statusHTML += '<span class="queue-indicator processing">å‡¦ç†ä¸­</span>';
                }
                
                if (queueLength > 0) {
                    statusHTML += `<span class="queue-indicator">å¾…æ©Ÿä¸­: ${queueLength}</span>`;
                }
                
                queueStatusDiv.innerHTML = statusHTML;
            }
        }
        
        function getSupportedMimeType() {
            const types = [
                'audio/wav',
                'audio/webm;codecs=opus',
                'audio/webm',
                'audio/ogg;codecs=opus',
                'audio/ogg'
            ];
            
            for (const type of types) {
                if (MediaRecorder.isTypeSupported(type)) {
                    console.log(`Using MIME type: ${type}`);
                    return type;
                }
            }
            
            throw new Error('No supported audio MIME type found');
        }
        
        // Copy all transcription text function
        function copyAllTranscriptions() {
            const copyBtn = document.getElementById('copyBtn');
            // Only select non-deleted transcription segments
            const transcriptionSegments = transcriptionDiv.querySelectorAll('.transcription-segment:not(.deleted) .text');
            
            if (transcriptionSegments.length === 0) {
                copyBtn.textContent = 'ã‚³ãƒ”ãƒ¼ã™ã‚‹å†…å®¹ãŒã‚ã‚Šã¾ã›ã‚“';
                setTimeout(() => {
                    copyBtn.textContent = 'å…¨æ–‡ã‚’ã‚³ãƒ”ãƒ¼';
                }, 2000);
                return;
            }
            
            // Collect all text from non-deleted transcription segments (excluding timestamps)
            const allText = Array.from(transcriptionSegments)
                .map(segment => segment.textContent.trim())
                .filter(text => text && text !== 'å‡¦ç†ä¸­...')
                .join('\n');
            
            // Copy to clipboard
            navigator.clipboard.writeText(allText).then(() => {
                // Show success feedback
                const originalText = copyBtn.textContent;
                copyBtn.textContent = 'ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸï¼';
                copyBtn.style.backgroundColor = '#333';
                copyBtn.style.color = 'white';
                copyBtn.style.borderColor = '#333';
                
                setTimeout(() => {
                    copyBtn.textContent = originalText;
                    copyBtn.style.backgroundColor = 'transparent';
                    copyBtn.style.color = '#666';
                    copyBtn.style.borderColor = '#e5e5e5';
                }, 2000);
            }).catch(err => {
                console.error('Failed to copy text:', err);
                copyBtn.textContent = 'ã‚³ãƒ”ãƒ¼ã«å¤±æ•—ã—ã¾ã—ãŸ';
                setTimeout(() => {
                    copyBtn.textContent = 'å…¨æ–‡ã‚’ã‚³ãƒ”ãƒ¼';
                }, 2000);
            });
        }
        
        // Initialize event listeners after all functions are defined
        startBtn.addEventListener('click', startRecording);
        stopBtn.addEventListener('click', stopRecording);
        document.getElementById('copyBtn').addEventListener('click', copyAllTranscriptions);
        
        // Add clear button functionality (optional)
        function addClearButton() {
            const controlsDiv = document.querySelector('.controls');
            const clearBtn = document.createElement('button');
            clearBtn.id = 'clearBtn';
            clearBtn.textContent = 'ã‚¯ãƒªã‚¢';
            clearBtn.style.backgroundColor = '#f5f5f5';
            clearBtn.style.color = '#666';
            clearBtn.addEventListener('click', () => {
                if (confirm('ã™ã¹ã¦ã®æ–‡å­—èµ·ã“ã—çµæœã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ')) {
                    transcriptionDiv.innerHTML = '<p style="color: #999; text-align: center; font-size: 14px;">éŒ²éŸ³ã‚’é–‹å§‹ã™ã‚‹ã¨ã€ã“ã“ã«æ–‡å­—èµ·ã“ã—çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚</p>';
                    clearSavedState();
                }
            });
            controlsDiv.appendChild(clearBtn);
        }
        
        // Load saved state on page load
        window.addEventListener('DOMContentLoaded', async () => {
            console.log('Page loaded, checking for saved state...');
            
            // Load saved transcriptions
            loadTranscriptions();
            
            // Load queue state (shows warning if items were lost)
            loadQueue();
            
            // Load saved state
            const savedState = loadState();
            if (savedState) {
                // Restore selected model
                const modelSelect = document.getElementById('modelSelect');
                if (savedState.selectedModel) {
                    modelSelect.value = savedState.selectedModel;
                }
                
                // Restore concurrency setting
                const concurrencySelect = document.getElementById('concurrencySelect');
                if (savedState.concurrency) {
                    concurrencySelect.value = savedState.concurrency;
                    MAX_CONCURRENT_REQUESTS = parseInt(savedState.concurrency);
                } else {
                    // Ensure default value is applied
                    MAX_CONCURRENT_REQUESTS = parseInt(concurrencySelect.value);
                }
                
                // Show info about restored state
                const timeDiff = Date.now() - new Date(savedState.timestamp).getTime();
                const minutes = Math.floor(timeDiff / 60000);
                if (minutes < 60) {
                    console.log(`State restored from ${minutes} minutes ago`);
                } else {
                    const hours = Math.floor(minutes / 60);
                    console.log(`State restored from ${hours} hours ago`);
                }
                
                // Check if recording was active before reload
                if (savedState.isRecording) {
                    console.log('Recording was active before reload, attempting to resume...');
                    
                    // Show notification to user
                    const notification = document.createElement('div');
                    notification.style.cssText = `
                        position: fixed;
                        top: 20px;
                        left: 50%;
                        transform: translateX(-50%);
                        background: #333;
                        color: white;
                        padding: 16px 24px;
                        border-radius: 8px;
                        box-shadow: 0 4px 12px rgba(0,0,0,0.2);
                        z-index: 1000;
                        font-size: 14px;
                        display: flex;
                        align-items: center;
                        gap: 12px;
                    `;
                    notification.innerHTML = `
                        <span class="recording-indicator" style="display: inline-block; width: 8px; height: 8px; background: #ff4444; border-radius: 50%; animation: pulse 1.5s infinite;"></span>
                        éŒ²éŸ³ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚ã‚¯ãƒªãƒƒã‚¯ã—ã¦å†é–‹ã—ã¦ãã ã•ã„ã€‚
                        <button id="resumeBtn" style="
                            margin-left: 12px;
                            padding: 6px 16px;
                            background: white;
                            color: #333;
                            border: none;
                            border-radius: 4px;
                            font-size: 13px;
                            cursor: pointer;
                            font-weight: 500;
                        ">å†é–‹</button>
                    `;
                    document.body.appendChild(notification);
                    
                    // Auto-resume recording when user clicks resume button
                    const resumeBtn = document.getElementById('resumeBtn');
                    resumeBtn.addEventListener('click', async () => {
                        try {
                            await startRecording();
                            notification.remove();
                            
                            // Show success message
                            const successMsg = document.createElement('div');
                            successMsg.style.cssText = notification.style.cssText;
                            successMsg.style.background = '#4CAF50';
                            successMsg.innerHTML = 'éŒ²éŸ³ã‚’å†é–‹ã—ã¾ã—ãŸ';
                            document.body.appendChild(successMsg);
                            setTimeout(() => successMsg.remove(), 3000);
                        } catch (error) {
                            console.error('Failed to resume recording:', error);
                            notification.innerHTML = `
                                <span style="color: #ff6b6b;">éŒ²éŸ³ã®å†é–‹ã«å¤±æ•—ã—ã¾ã—ãŸã€‚</span>
                                <button onclick="this.parentElement.remove()" style="
                                    margin-left: 12px;
                                    padding: 6px 16px;
                                    background: white;
                                    color: #333;
                                    border: none;
                                    border-radius: 4px;
                                    font-size: 13px;
                                    cursor: pointer;
                                ">é–‰ã˜ã‚‹</button>
                            `;
                        }
                    });
                    
                    // Also try to auto-resume if permissions were previously granted
                    try {
                        // Check if we have permission
                        const permissionStatus = await navigator.permissions.query({ name: 'microphone' });
                        if (permissionStatus.state === 'granted') {
                            console.log('Microphone permission already granted, auto-resuming...');
                            // Add slight delay to ensure UI is ready
                            setTimeout(async () => {
                                try {
                                    await startRecording();
                                    notification.remove();
                                    
                                    // Show auto-resume success
                                    const autoResumeMsg = document.createElement('div');
                                    autoResumeMsg.style.cssText = notification.style.cssText;
                                    autoResumeMsg.style.background = '#4CAF50';
                                    autoResumeMsg.innerHTML = 'éŒ²éŸ³ã‚’è‡ªå‹•çš„ã«å†é–‹ã—ã¾ã—ãŸ';
                                    document.body.appendChild(autoResumeMsg);
                                    setTimeout(() => autoResumeMsg.remove(), 3000);
                                } catch (error) {
                                    console.log('Auto-resume failed, user action required');
                                }
                            }, 500);
                        }
                    } catch (error) {
                        console.log('Permission check not supported');
                    }
                    
                    // Change start button appearance to indicate resume is needed
                    startBtn.style.animation = 'pulse 2s infinite';
                    startBtn.style.background = '#ff4444';
                    startBtn.textContent = 'éŒ²éŸ³ã‚’å†é–‹';
                    
                    // Reset button appearance when clicked
                    startBtn.addEventListener('click', function resetButton() {
                        startBtn.style.animation = '';
                        startBtn.style.background = '';
                        startBtn.textContent = 'éŒ²éŸ³é–‹å§‹';
                        startBtn.removeEventListener('click', resetButton);
                        if (notification.parentElement) {
                            notification.remove();
                        }
                    }, { once: true });
                }
            }
            
            // Add clear button
            addClearButton();
        });
        
        // Save state when model selection changes
        document.getElementById('modelSelect').addEventListener('change', saveState);
        
        // Update MAX_CONCURRENT_REQUESTS when concurrency selection changes
        document.getElementById('concurrencySelect').addEventListener('change', (e) => {
            MAX_CONCURRENT_REQUESTS = parseInt(e.target.value);
            console.log('Concurrency limit updated to:', MAX_CONCURRENT_REQUESTS);
            saveState();
            
            // If we have pending items, immediately process more
            if (processingQueue.length > 0 && concurrentRequests < MAX_CONCURRENT_REQUESTS) {
                // Process additional segments up to the new limit
                for (let i = concurrentRequests; i < MAX_CONCURRENT_REQUESTS && processingQueue.length > 0; i++) {
                    setTimeout(() => processQueue(), 0);
                }
            }
        });
        
        // Save state periodically while recording
        setInterval(() => {
            if (isRecording) {
                saveState();
            }
        }, 30000); // Every 30 seconds
        
        // Save state before page unload
        window.addEventListener('beforeunload', () => {
            saveState();
            saveTranscriptions();
            saveQueue();
        });
    </script>
</body>
</html>